%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%        Annotated example of a homework submission in LaTeX         %%
%%               (note: LaTeX comments begin with a %)                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Build this document with
%%
%%   pdflatex annotated_homework_example.tex
%%
%% or
%%
%%   latex annotated_homework_example.tex
%%   dvips annotated_homework_example.dvi
%%   ps2pdf annotated_homework_example.ps
%%
%% The result will be annotated_homework_example.pdf.
%%
%% IMPORTANT NOTE: Because this sample uses cross references, you must
%% build it *TWICE*. LaTeX finds all of the cross reference TARGETS first
%% and then, on the second run, plugs them all in to the appropriate
%% places.
%%
%% (note: any good LaTeX-friendly text editor will have these build
%%  lines built into it)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PREAMBLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Every LaTeX document starts with a \documentclass line. The
%% document class sets up the most prominent features of the document
%% style (e.g., page styles, font, etc.).
%%
%% Standard document classes include article, book, and report.
%%
%%   *) The article document class is the most commonly used for short
%%      documents. It provides sectioning commands and flexible titling
%%      options.
%%
%%   *) The report document class gives you chapters and puts the title
%%      information centered on a separate unnumbered page.
%%
%%   *) The book document class gives you the ability to make parts and
%%      chapters and is usually used in two-sided printing mode (e.g.,
%%      page numbering alternates from being on the top left to top
%%      right of the page rather than in the bottom center).
%%
%% More advanced document classes include memoir, which is a drop-in
%% replacement for the standard classes that has many other features as
%% well. It folds in a rich feature set from some of the most popular
%% packages (packages are explained below).
%%
%% Optional arguments to LaTeX macros come in square brackets. In this
%% case, [10pt] tells the article class that we want 10 point font.
\documentclass[9pt]{article}
\pagestyle{empty}

%% The default margins for article may seem large to you. They are
%% designed to look pleasant on a page. To use 1 inch margins instead,
%% call the geometry package with this line:
\usepackage{savetrees}

%redundant but w.e
\usepackage{fullpage,latexsym,amsthm,amsmath,amsfonts,algorithmic,graphicx,subfigure,color,verbatim,epsfig,fancyhdr,}

% Define new commands.
\DeclareMathOperator*{\E}{\mbox{\text{E}}}

\newcommand{\pref}[1]{{(\ref{#1})}}
\newcommand{\bmat}[1]{{\color{red}{#1}}}

\newtheorem{defn}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{question}{Question}[section]
\newtheorem{proposition}{Proposition}[section]


%\oddsidemargin=-.1in
%\evensidemargin=0in
%\topmargin=-.75in


% The graphicx package lets us include images with \includegraphics
\usepackage{graphicx}
\usepackage{epstopdf}
% The caption package lets us customize captions when we want to make
% formal figures and tables
\usepackage{multicol}
\usepackage{amsmath, amsthm, amssymb}

%%%%%%%%%%%%%%%%%%%%%%%% MAIN DOCUMENT CONTENT %%%%%%%%%%%%%%%%%%%%%%%%%

% We surround our main content with a ``document'' ``environment.'' This
% marks the end of our LaTeX setup and the start of our document output.
%
% All ``environments'' are identified by \begin{...} and \end{...}
% macros.
\begin{document}
\begin{multicols}{3}
\begin{tiny}
% This is a note about running LaTeX twice. It's centered, 3/4 the width
% of the printed type block, and it has a box around it. You can remove
% it.
%\centerline{\fbox{\parbox{0.75\columnwidth}{\textbf{SPECIAL NOTE:} If
%you notice a few ``\textbf{??}'' in funny places in your document (e.g.,
%the footer may say ``\textbf{Page 1 of ??}''), then run \LaTeX{} again.
%That information was not available in the first pass, but it will be
%filled in properly in the second pass.}}}

% Creates a section header with ``Manually Numbered Answers'' in it
\section{Chapter 2 Problems}

%%
%% Chapter 2, problems 2, 4, 10, 12, 15, 26, and 32.
%%

% Creates a list

        % Pass optional items in square brackets to each \item macro.
        % Those optional items get displayed as the label for that item.
\section* {Find a general inequality for $H(X)$ vs $H(f(X))$}
                %
                % An enumerate list will automatically count each item
                % for you. Pass it the type of label you want (e.g., 1.
                % or a) or i) or ...). See below for another example.
                          let $y=g(x)$. $p(y)=\sum_{x:y=g(x)}p(x)$ Meaning sum over all $x$ given that $y=g(x)$. It is pretty clear that if there is a 1-1 mapping then $p(x)=p(y)$. 
                          
                          Consider a set of $x$ that maps to a single $y$. For this set $\sum_{x:y=g(x)}p(x)\ log\ p(x) \leq \sum_{x:y=g(x)}p(x)\ log\ p(y) = p(y)\ log\ p(y)$ since $p(y)\geq p(x)$. Now extending the argument to the entire range of $X$ and $Y$ we obtain:
          \(
          p(x)\leq \sum_{x:y=g(x)}p(x)=p(y)\) and \(
          H(X)=-\sum_{x}p(x)\ log\ p(x)
          =-\sum_{y}\sum_{x:y=g(x)}p(x)\ log\ p(x)
          \geq -\sum_{y}p(y)\ log\ p(y)
          =H(Y)
          \)

          {\bf a.)} $H(X)$ vs $H(Y=2^{X})$\\
           $Y$ and $X$ are 1-1 so we have the special case of equality.

          {\bf b.)} $H(X)$ vs $H(Y=cos(X))$ \\
            There are multiple values of $X$ that could map to single values of $Y$ with this function. Thus we can only guarentee the general case where $H(X) \geq H(Y)$

\section*{ Show that $H(f(X))\leq H(X)$}
                  {\bf a.)} $H(X,g(X))=H(X) + H(g(X) | X)$\\
                    This is justified by the chain rule.
                    
                  {\bf b.)} $=H(X)$ \\
                    $H(g(X)|X)=0$ because $g(X)$ is known for any value of $X$. 
                    Thus $H(g(X)|X)=\sum_{x}p(x)\ H(g(X)|X=x)=\sum_{x}0=0$ leaving
                    $H(X)$
                    
                  {\bf c.)} $H(X,g(X))=H(g(X))+H(X|g(X))$ \\
                    Just a different application of the chain rule.
                    
                  {\bf d.)} $\geq H(g(X))$\\
                   $H(X|g(X))\geq0$ and is equal to 0 when $g(X)$ gives an 
                   unambiguous mapping to one $X$. Otherwise it will be 
                   greater than 0. Thus $H(g(X))\leq H(X,g(X))$ 

\section*{ $H$ of disjoint mixture
        \(
         X=\begin{cases} X_1 & w/p\ \alpha \\
         X_2 & w/p\ (1-\alpha) \end{cases} 
        \)}
        
           {\bf a.)} What is $H(X)$ in term of $H(X_1), H(X_2), \alpha$? 
              \(
              	\theta = f(X) = \begin{cases}1 & when X=X_{1}\\ 2 & when X = X_{2} \end{cases}\) and \(
	H(X) = H(X,f(X)) = H(\theta) + H(X|\theta)\ 
	= H(\theta)+p(\theta = 1)\ H(X|\theta=1)+p(\theta=2)\ H(X|\theta=2)\ 
	= H(\alpha) + \alpha\ H(X_{1}) + (1-\alpha)\ H(X_{2})
              \)
              where \(H(\alpha) = -\alpha\ log\ \alpha - (1 - \alpha )\ log\ (1-\alpha)\)
             
            %\item {\bf TODO!!!}
                  
\section*{Application of $H$ and $I$}
       We are given that  \( p(x,y)=\left[ \frac{1}{3},\frac{1}{3},0,\frac{1}{3} \right] \) Find the following:
       
            {\bf a.)}  \(H(X), H(Y)\)
              \(
                H(X) = H(\frac{2}{3},\frac{1}{3})
                = \frac{2}{3} log(\frac{3}{2}) + \frac{1}{3}log(3)\\
                \approx 0.9183 bits \) and \(
                H(Y) =  H(\frac{2}{3},\frac{1}{3})\
                \approx 0.9183 bits
              \)
              
            {\bf b.)}
              \(
                H(X|Y) = \frac{1}{3}H(X|Y=0) + \frac{2}{3}H(X|Y=1)
                =\frac{1}{3}H(\frac{1}{2},\frac {1}{2})+\frac{2}{3}H(0,1)
                =\frac{1}{3}1+\frac{2}{3}0 
                = \frac{1}{3} bits \) 
                
                \(
                H(Y|X) = \frac{2}{3}H(Y|X=0)+\frac{1}{3}H(Y|X=1)
                =\frac{2}{3}H(\frac{1}{2},\frac{1}{2}) + \frac{1}{3}H(0,1)
                =\frac{2}{3} bits
              \)
              
            {\bf c.)}
           	 \(
            		H(X,Y)=H(X)+H(Y|X)
			\approx 1.585 bits
          	  \)
          	  
            {\bf d.)} \(H(Y) - H(Y|X) \approx 0.251 bits\)
            	
            {\bf e.)} \( I(X;Y) =H(Y) - H(Y|X) \approx 0.251 bits
		\)
		
            {\bf f.}  Imagine a two circle venn. $H(X,Y)$ represents the overlapped portion plus the two independent portions. $H(X)$ is one circle and $H(Y)$ is the other. The overlapped portion is represented by either $I(X;Y)$ or $H(Y)-H(Y|X)$.
          
\section*{Mutual information and Markov chain} Markov chains: $X_{1}\rightarrow X_{2}\rightarrow \cdots \rightarrow X_{n}$, 
        		have joint probabilities of the following form: 
		\(
			p(x_{1},x_{2},\ldots,x_{n})=p(x_{1})p(x_{2}|x_{1})\cdots p(x_{n}|x_{n-1})
		\)
		This means that for mutual information we can do some nice reduction.
		first let \( X_{3},\ldots,X_{n} = Y\) then you get \(
			I(X_{1};X_{2},Y) = I(X_{1};X_{2})+I(X_{1};Y|X_{2})  \)
		Now since this is a markov chain, and we know that $X_{1}$ and $Y$ are completely independent
		we can say that $I(X_{1};Y|X_{2})=0$ since $Y|X_{2}$ is just a subset of $Y$ which as noted
		previously is entirely independent of $X_{1}$. Thus the whole equation simplifies to $I(X_{1};X_{2})$

\section*{Using concavity to prove stuff}
			{\bf a.)} $ln(x) \leq x-1$ for $0\leq x \leq \infty$\\
				If the second derivative of a function is positive, that means that it is concave up, or convex. If it is always greater than 0, it will never not be convex. If it could be zero on the interval, then we can't make this claim, and the converse isn't necessarily true.
				Let \(f(x)=x-1-ln\ x \) then 
				\(
				 f'(x)= 1-\frac{1}{x}\) and \(
				 f''(x)=\frac{1}{x^2} > 0
				\)
				 so we can say that $f(x)$ is strictly convex and a local minimum is also a global minimum. To find the local minimum we just set $f'(x)=0$ and get $x=1$. So $f(x) \geq f(1)$ which means that $x-1-ln\ x \geq 1-1-ln\ 1 = 0$. Showing that $x-1 \geq ln\ x$ on the interval $(0,\infty)$.
				 
			{\bf b.)} Justify the following steps. 
				Let A be the set of $x$ such that $p(x) > 0$
				\(
				-D_e(p||q)=\sum_{x\in A} p(x) ln \frac{q(x)}{p(x)}
				\leq \sum_{x\in A} p(x)\left( \frac{q(x)}{p(x)}-1\right)
				=\sum_{x\in A} q(x) - \sum_{x\in A} p(x)
				\leq 0
				\)
				The first step follows from the definition of D, the second step follows from the inequality $ln\ t \leq t - 1$, the third from expanding the sum, and the last step from the fact that the $q(A) \leq 1$ and $p(A)=1$.
				
			{\bf c.)} Conditions for equality?
			We have the inequality $ln\ t \leq t-1$ from a previous problem, and showed equality iff $t=1$. Therefore we have equality if $\frac{q(x)}{p(x)}=1$ for all $x\in A$. This implies that $p(x)=q(x)$ for all $x$ and then we'll have equality in the last step as well. 	
          
\section*{Calculating and estimating error}
        \(P(X,Y) = \left( \begin{array}{ccc}
\frac{1}{6} & \frac{1}{12} & \frac{1}{12} \\
\frac{1}{12} & \frac{1}{6} & \frac{1}{12} \\
\frac{1}{12} & \frac{1}{12} & \frac{1}{6} \end{array} \right)\)
 where $X$ is indexed $[a,b,c]$ and $Y$ is indexed $[1,2,3]$. Let $\hat{X}(Y)$  be an estimator for $X$ based on $Y$ and let $P_e=Pr\{\hat{X}(Y)\neq X\}$

        {\bf a.)}
        	From inspection we see that 
        	\(\hat{X}(y)=\left\{ \begin{array}{cc}
        		1 & y=a\\
        		2 & y=b\\
        		3 & y=c \end{array}\right. \)
	and the associated $P_e$ is the sum of 
	\[
	P(1,b),  P(1,c), P(2,a),
	 P(2,c), P(3,a), P(3,b) \] Therefore $P_e=\frac{1}{2}$
        
        {\bf b.)}
        	From Fano's inequality we know \(P_e \geq \frac{H(X|Y)-1}{log\ |\chi|}\). \(
        	H(X|Y)= H(X|Y=a)Pr\{y=a\}+H(X|Y=b)Pr\{y=b\}+H(X|Y=c)Pr\{y=c\}
        	=H\left(\frac{1}{2},\frac{1}{4},\frac{1}{4}\right)Pr\{y=a\}+H\left(\frac{1}{2},\frac{1}{4},\frac{1}{4}\right)Pr\{y=b\}+H\left(\frac{1}{2},\frac{1}{4},\frac{1}{4}\right)Pr\{y=c\}
        	=H\left(\frac{1}{2},\frac{1}{4},\frac{1}{4}\right)\left( Pr\{y=a\}+Pr\{y=b\}+Pr\{y=c\}\right)
        	=H\left(\frac{1}{2},\frac{1}{4},\frac{1}{4}\right)
        	=1.5
        	\) bits. 
        	Our estimator of error $P_e\geq \frac{1.5-1}{log\ 3}=0.316$ is not very close to Fano's bound in this form. Since $\hat{X}\in \chi$ we can use the stronger form of Fano's inequality 
        	\( P_e \geq \frac{H(X|Y)-1}{log(|\chi|-1)}  \) to get \( P_e \geq \frac{1.5-1}{log\ 2}=\frac{1}{2} \) which is a great bound on our actual $P_e$.


\section*{3.1}

\subsection*{(a)}

(Markov's Inequality) For non-negative random variable $X$ and  $t>0$,
Show
\begin{equation}
    \Pr\{X \geq t\} \leq \frac{\E X}{t} \label{eq: markovs ineq}
\end{equation}
And provide an example random variable for with the above condition is met
with equality.

\proof

\begin{eqnarray}
    \E X &= \int_{-\infty}^\infty x f(x)dx &\text{(Defn of expectation.)}
        \nonumber \\
    &\geq \int_t^\infty x f(x)dx &\text{(Integral over subdomain)} 
        \nonumber \\
    &\geq \int_t^\infty t f(x)dx &\text{($x \geq t$ in integral)} 
        \nonumber \\
    &= t \Pr\{X \geq t\}  &\text{($t$ constant in integral)}
        \label{eq: 3.1a QED}
\end{eqnarray}
The last equation \pref{eq: 3.1a QED} proves Markov's Inequality
\pref{eq: markovs ineq}.

\begin{flushright}
    $\Box$
\end{flushright}

Let $X$ be a \emph{random variable} which has value zero with probability
one.
The expected value of $X$ is zero and thus, 
\pref{eq: markovs ineq} is always an equality.

\subsection*{(b)}

(Chebychev's Inequality) Let $Y$ be a random variable with mean $\mu$ and
variance $\sigma^2$. By letting $X = (Y-\mu)^2$, show that for
$\varepsilon > 0$
\begin{equation}
    \Pr \{(Y-\mu)^2 > \varepsilon\} \leq \frac{\sigma^2}{\varepsilon^2}
    \label{eq: chebychevs ineq}
\end{equation}

\proof

\begin{eqnarray}
    \Pr\{(Y-\mu)^2 > \varepsilon\} &\leq& \Pr\{(Y-\mu)^2 \geq \varepsilon^2\}
        \nonumber \\
        &=& \Pr\{|Y-\mu| \geq \varepsilon\}
        \label{eq: equate probs} \\
    \Pr \{(Y-\mu)^2 > \varepsilon^2\} &\leq& \frac{\E \big[(Y-\mu)^2\big]}{\varepsilon^2}
        \label{eq:instatiate 3.1a}
\end{eqnarray}
The final inequality coming from an application of Markov's Inequality
\pref{eq: markovs ineq} and the derived inequality \pref{eq: equate probs}.
The numerator $E[(Y-\mu)^2]$ is equal to $\sigma^2$, from the definition
of variance and the fact that $\mu = E[Y]$. Thus we are left with
Chebyshev's inequality \pref{eq: chebychevs ineq}.
\begin{flushright}
    $\Box$
\end{flushright}

\subsection*{(c)}

(The weak law of large numbers.) Let $Z_1,Z_2,\dots,Z_n$ be a sequence of
i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Let
$\overline{Z_n} = \frac{1}{n}\sum{i=1,2,\dots,n}Z_i$ be the sample mean.
Show 
\begin{equation}
    \Pr \{|\overline{Z_n} - \mu| > \varepsilon\} \leq
    \frac{\sigma^2}{n\varepsilon^2}
    \label{eq: weak loln}
\end{equation}

\proof

$\overline{Z_n}$ is a random variable with mean $\mu$ and variance
$\frac{\sigma^2}{n}$. This is easily computed because expectation
distributes across linear combinations, 
\begin{equation*}
    E\bigg[\frac{1}{n}\sum_{i=1}^{n}Z_i\bigg] 
    = \frac{1}{n} \sum_{i=0}^{n} E[Z_i] = \frac{n\mu}{n} = \mu
\end{equation*}
, and variance distributes across linear combinations, squaring the
leading coefficients,
\begin{equation*}
    Var\bigg[\frac{1}{n}\sum_{i=1}^{n}Z_i\bigg]
    = \frac{1}{n^2}\sum_{i=0}^{n} Var[Z_i] 
    = \frac{n\sigma^2}{n^2} 
    = \frac{\sigma^2}{n}
\end{equation*}
. The weak law of large numbers \pref{eq: weak loln} comes from applying
Chebychev's inequality \pref{eq: chebychevs ineq} to $\overline{Z_n}$.
\begin{flushright}
    $\Box$
\end{flushright}
\section*{3.2}

Let $(X_i,Y_i)$ be i.i.d. $\sim p(x,y)$. We form the log likelihood ratio
of the hypothesis that $X$ and $Y$ are independent vs. the hypothesis that
$X$ and $Y$ are dependent. What is the limit of
\begin{equation}
    \frac{1}{n}\log \frac{p(X^n)p(Y^n)}p(X^n,Y^n)
    \label{eq: normalized llr}
\end{equation}
$Solution$:
\begin{eqnarray}
    \frac{1}{n}\log \frac{p(X^n)p(Y^n)}{p(X^n,Y^n)}
    &=& \frac{1}{n} \log \prod_{i=1}^{n}\frac{p(X_i)p(Y_i)}{p(X_i,Y_i)}
    \label{eq: expand probs} \\
    &=& \frac{1}{n} \sum_{i=1}^{n} \log \frac{p(X_i)p(Y_i)}{p(X_i,Y_i)}
    \label{eq: log of prods} \\
    &\rightarrow& \E \log \frac{p(X_i)p(Y_i)}{p(X_i,Y_i)} 
    \label{eq: sum convergence} \\
    &=& -I(X;Y) 
    \label{eq: from MI defn}\\
\end{eqnarray}

The initial equality \pref{eq: expand probs} comes from the initial
independence assumption.
The convergence \pref{eq: sum convergence} comes from \pref{eq: log of
prods} and the (strong?) law of large numbers. 
The equality \pref{eq: from MI defn} comes from the
definition of mutual information.

This implies convergence of the likelihood ratio
$\frac{p(X^n)p(Y^n)}{p(X^n,Y^n)} \rightarrow 2^{-nI(X;Y)}$ which is 1 iff
$X$ and $Y$ are independent.

\section*{3.6}

Let $X_1,X_2,\dots$ be i.i.d. drawn according to probability mass function
$p(x)$. Find the limit
\begin{equation}
    \lim_{n \to \infty} (p(X_1,X_2,\dots,X_n))^{\frac{1}{n}}
    \label{eq: aep like limit}
\end{equation}
$Solution$:
\begin{eqnarray}
    P_n &=& (p(X_1,X_2,\dots,X_n))^{\frac{1}{n}}
        \label{eq: defn P_n} \\
    \lim_{n\to \infty} P_n &=& 2^{lim_{n\to\infty}log\ P_n}
        \label{eq: limit of composition} \\
    &=& 2^{-H(X)},
        \label{eq: P_n limit} 
    \end{eqnarray} 
    The convergence of the exponent in \pref{eq: limit of composition} to
    $-H(X)$ comes from the definition of the AEP.
    The above result assumes that $H(X)$ exists.

\section*{3.11}

\subsection*{(a)}

Given any two sets $A$, $B$ such that $\Pr{A} > 1-\varepsilon_1$ and
$\Pr{B} > 1-\varepsilon_2$, show that $\Pr(A\cap B) > 1 - \varepsilon_1 -
\varepsilon_2$. Hence
\begin{equation}
    \Pr(A_\varepsilon^{(n)} \cap B_\delta^{(n)}) > 1 - \varepsilon - \delta
    \label{eq: 3.11(a) conclusion}
\end{equation}
$Solution$:
\begin{eqnarray}
    Pr(A\cap B) &=& Pr(A) + Pr(B) - Pr(A\cup B)
    \label{eq: pr A intersect B} \\
    &>& 2 - \varepsilon_1 - \varepsilon_2 - Pr(A\cup B)
    \label{eq: sub hypotheses} \\
    &>& 1 - \varepsilon_1 - \varepsilon_2
    \label{eq: 3.11a}
\end{eqnarray}

\subsection*{(b)}
\begin{itemize}
    \item (3.34): Inequality \ref{eq: 3.11(a) conclusion} from part (a).
    \item (3.35): Definition of the probability of an event.
    \item (3.36): By definition, for $x^n \in A_\varepsilon^{(n)}$
        $p(x^n) \leq 2^{-n(H - \varepsilon)}$.
    \item (3.37): Sum over a constant summand (not dependant on the
        sequence $x^n$).
    \item (3.38): The intersection's cardinality is no greater than the cardinality of either set individually.
\end{itemize}


Doubly stochastic (square) matrices are when rows and columns each sum to one. \textbf{(a)}  Let b = aP show H(b) $\ge$ H(a). \textbf{(b)} Show that a stationary distribution $\mu$ for a doubly stochastic matrix P is the uniform distribution. \textbf{(c)} Conversely, prove that if the uniform distribution is a stationary distribution for a Markov transition matrix P , then P is doubly stochastic.

\[H(b) - H(a) = -\sum_j b_i \log (b_i) + \sum_i a_i \log (a_i)  \]
\[= - \sum_j \sum_i a_iP_{ij} \log(\sum_k a_kP_{kj})+\sum_t a_i \log (a_i) \]	
\[= \sum_{ij}a_iP_{ij} \log(\frac{ a_i }{\sum_{k} a_kP_{kj}}) (cite mult by 1)\]	
\[ \ge (\sum_{ij}a_iP_{ij}) \log(\frac{\sum_{ij} a_i }{\sum_{ij} b_j }) (cite log sum)\]	

b) If the matrix is doubly stochastic, the substituting $u_i$ = $1\over{m}$, we can easily see it satisfies $u=uP$
c) if the uniform is a stationary distribution, then $\frac{1}{m} = u_i = \sum_{j}u_{j}P_{ij}$ = $ \frac{1}{m}\sum_jP{ji}$ or $\sum_jP{ji}=1$ or that the matrix is doubly stochastic.


\section*{monotonicity of entropy per element}
For a stationary stochastic process $X_1, X_2, \cdots , X_n$, show that
\[ (chainRule)  \frac{H(X_1,X_2,\cdots,X_n)}{n} = \frac{\sum_{i=1}^n H(X_i | X^{i-1})}{n} \]
\[ \frac{(H(X_n | X^{n-1}) + \sum_{i=1}^{n-1} H(X_i |X^{i-1}) } {n} \]
\[ (cite??) \frac{H(X_n | X^{n-1}) + H(X_1,X_2,\cdots,X_{n-1}) } {n} \]
From stationarity it follows that for all $1\le i \le n$ $->$ $H(X_n | X^{n-1}) \le H(X_i | X^{i-1}) $
which further implies, by averaging over all the i`s, that,
\[ H(X_n | X^{n-1}) \le \frac{ \sum_{i=1}^{n-1} H(X_i | X^{i-1})} {n-1} \]
\[ \frac{ H(X_i,X_2,\cdots,X_{n-1})} {n-1} \]
combining past two results yeilds, 
\[ \frac{H(X_1,X_2,\cdots,X_n)}{n} \le \frac{1}{n} [ \frac{ H(X_1,X_2,\cdots,X_n) } {n-1}\]
\[ + H(X_1,X_2,\cdots,X_{n-1}) ]  = \frac{H(X_1,X_2,\cdots,X_{n-1})}{n-1} \]
\\
b) By stationarity we have $\forall 1 \le i \le n$,
\[ H(X_n | X^{n-1} ) \le H(X_i | X^{i-1}) (whichimplies)\]
\[H(X_n | X^{n-1} = \frac{\sum_{i=1}^n H(X_n | X^{n-1} } {n} \]
\[ \frac{\sum_{i=1}^n H(X_i | X^{i-1} } {n} = \frac{H(X_1,X_2,\cdots,X_n)}{n}\]

\section*{Entropy rates of Markov chains}

Let matrix P = $[ (1-p_{01}, p_{01}), (p_{10}, 1-p_{10}]$ 
\textbf{(a)} The stationary distribution is: $u_0=\frac{p_{10}}{p_{01}+p_{10}}$, $u_1=\frac{p_{01}}{p_{01}+p_{10}}$
Therefore the entropy is: \( H(X_2 \mid X_1) = u_0H(p_{01})+u_1H(p_{10}) = \frac{p_{10}H(p_{01})+ p_{01}H(p_{10})}{p_{01}+p_{10}}. \)

\textbf{(b)} The entropy rate is at most 1 bit because process has only two states. This rate can be achieved iff \(p_{01}=p_{10}=1/2 \), in which case the process is i.i.d with \( \Pr(X_i = 0) =\Pr(X_i=1) = 1/2 \)

Let matrix P = $[ (1-p, p), (1, 0)]$ 
\textbf{(c)} special case if the general two-state Markov chain, the entropy rate is \( H(X_2 | X_1)=u_0H(p)+u_1H(1)=\frac{H(p)}{p+1} = \frac{-p\log(p)-(1-p)\log(1-p)}{1+p} \)
\textbf{(e)} By calculus: \( p=\frac{(3-5^{1/2})}{2}=0.382.\) the max value is \(H(p)=H(1-p)=H(\frac{5^{1/2}-1}{2}) = 0.694bits\)
note that \( \frac{(5^{1/2}-1)}{2} =.618\) is (the reciprocal of) the Golden Ratio.

\section*{Show for markov chain become more difficult to recover as the future $X_n$ unfolds. $H (X_0|Xn_) \ge H (X_0|X_{n?1})$.}
Solution: initial conditions. For a Markov chain, by the data processing theorem we have $I(X_0  \: X_{n-1}) \ge I(X_0 \ : X_n)$ Therefore $H(X_0) - H(X_0 | X_{n-1}) \ge H(X_0) - H(X_0 | X_n)$ or $H(X_0 | X_n)$  increases with n

\section*{Let	. . . ,$ X_{-1}, X_0, X_1$, . . . be	a stationary (not necessarily Markov) stochastic process.}
Solution: stationary processes A) $H(x_n | X_0) = H(X_{-n} | X_0).$ True cause: $H(X_n | X_0) = H(X_n,X_0) - H(X_0)$ and $H(X_{-n} | X_0) = H(X_{-n},X_0) - H(X_0)$ by stationarity.
b) \(H(X_n\mid X_0) \ge H(X_{n-1} \mid X_0). \) \textbf{Not true} in general. Thous is true for first order markov chains. A simple counterexample is a periodic process with period n. Let $X_0,X_1,X_2,\cdots,X_{n-1}$ be i.i.d. uniformly distributed binary random variables and let \(X_k =X_{k-n}\) for \( k \ge n \) In this cases, $H(X_n \mid X_0)$ and $H(X_{n-1} \mid X_0) = 1$, contradicting the statement $H(X_n | X_0) \ge H(X_{n-1} \mid X_0)$. C) \(H(X_n \mid X_1^{n-1}, X_{n+1}) \) is non-increasing in n. \textbf{true}, since by stationarity \(H(X_n \mid X_1^{n-1}, X_{n+1}) \) = \(H(X_{n+1} \mid X_2^{n}, X_{n+2}) \)  \( \ge H(X_{n+1} \mid X_1^{n}, X_{n+2}) \) where the inequality follows from the fact that conditioning reduces entropy.


\end{tiny}

\end{multicols}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%