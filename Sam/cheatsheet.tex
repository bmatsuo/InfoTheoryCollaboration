\documentclass[10pt]{article}
\usepackage{fullpage,latexsym,amsthm,amsmath,algorithmic,graphicx,subfigure,verbatim,epsfig,fancyhdr}

% Define new commands.

\newcommand{\pref}[1]{{(\ref{#1})}}
\newcommand{\annotate}[1]{{\color{red}{#1}}}

\newtheorem{defn}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{question}{Question}[section]
\newtheorem{proposition}{Proposition}[section]
\usepackage{multicol}
\usepackage{savetrees}
% Begin the actual document content here.

\begin{document}

% Setup document header/footer and adjust spacing accordingly.
\pagestyle{fancy}
\lhead{Cheatsheet - EE 253 Fall 2010}
\rhead{Sam Wood (iamsamwood@gmail.com)}
\rfoot{\today}
\renewcommand\headheight{15pt}
\renewcommand\footrulewidth{0.4pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\headsep}{0pt}
\setlength{\topskip}{15pt}
\setlength{\topmargin}{0pt}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
%end the header thing.
\noindent
\textit{Entropy Rate of Typewriter:} All letters are equally likely. $m$ letters, sequence has length $n$, $m^n$ sequences with equal probability ($p(x^n) = \frac{1}{m^n}$): $H(X^n) = \sum_{x^n} p(x^n) log(\frac{1}{p(x^n)}) =  \frac{m^n}{m^n}log(m^n) = n*log(m)$; $H(\chi) lim_{n \to \infty} \frac{1}{n} H(X^n) = lim_{n \to \infty} log(m) = log(m)$ bits per symbol.
\\\\
\textit{Entropy Rate of i.i.d. RVs:} $X_i$ is i.i.d: $H(\chi) = lim_{n \to \infty} \frac{1}{n} H(X^n) = lim_{n \to \infty} \frac{nH(X_1)}{n} = H(X_1)$
\\\\
\textit{Entropy Rate of Independent RVs:} $X_i$s independent $\implies H(X^n) = \sum_i^n H(X_i)$; $H(\chi) = lim_{n \to \infty} \frac{1}{n} \sum_i^n H(X_i) $; Since $H(X_i)$s are different, this entropy does not exist (and may not converge), i.e. $P(X_i = 1) = r_i, P(X_i = 0) = 1 - r_i$, where $r_i$ is not a constant value and is a function of $i$. $r_i = 0.5$ if $2k < log(log(i)) \leq 2k + 1$, $r_i = 0$ if $2k + 1 < log(log(i)) \leq 2k +2 $ for some constant $k$. $H(X_i)$ switches between 1 and 0, $H(\chi)$ changes with $n$.
\\\\
\textit{Shuffling Does Not Decrease Entropy} Let $X =$ location of cards, $T =$ shuffle operation of the cards, where $X$ and $T$ are independent. Show $H(TX) \geq H(X)$: $H(TX) \geq H(TX|T)$ (conditioning reduces entropy) $= H(T^{-1}TX|T)$ (if shuffle operation is known, so is inverse) = $H(X|T) = H(X)$ (since $X$ and $T$ are independent).
\\\\
\textit{Source Entropy per Unit Time} Given a discrete memoryless source with alphabet $1 \gets p_1, 2 \gets p_2$ with symbol duration $1 \gets 1$, $2 \gets 2$. Maximize the source entropy per nit time: $f(p_1) = \frac{H(p_1)}{E(l(x))}$ (where $l$ is duration), $E(l(x)) = T(p_1) = 1*p_1 + 2*p_2 = 1*p_1 + 2(1-p_1) = 2 - p_1$, $\frac{H(p_1)}{T(p_1)} = \frac{-p_1 log(p_1) - (1-p_1)log(1-p_1)}{2-p_1}$. Note if $p_1 = 1 \to f(p_1) = 0$, $p_1 = 2 \to f(p_1) = 0$ (therefore take derivative): $\frac{\partial f(p_1)}{\partial p_1} = \frac{ T(p_1)\frac{\partial H(p_1)}{\partial p_1} - H(p_1)\frac{\partial T(p_1)}{\partial p_1}}{(T(p_1))^2} = \frac{(2-p_1)[-log(p_1) -1 + log(1-p_1) - 1] - H(p_1)(-1)}{(T(p_1))^2}=0$; $ln(1-p_1) - 2ln(p_1) = 0 \to ln(1-p_1) = 2ln(p_1) = ln(p_1^2)$, $1-p_1 = p_1^2 \to p_1^2 + p_1 - 1 = 0 \to p_1 = \frac{1}{2}(\sqrt(5) - 1), p_2 = 1 - p_1 = (\frac{3}{2} - \frac{1}{2}\sqrt(5))$; $\frac{H(p_1)}{T(p_1)} = 0.69424$ bits. 

% Add a bibliography.
% \bibliographystyle{plain}
% \bibliography{brlen}
\end{document}
