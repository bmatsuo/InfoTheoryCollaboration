\documentclass[10pt]{article}

\usepackage{fullpage,latexsym,amsthm,amsmath,amsfonts,algorithmic,graphicx,subfigure,color,verbatim,epsfig,fancyhdr}
\pagestyle{empty}
% Define new commands.
\DeclareMathOperator*{\E}{\mbox{\text{E}}}

\newcommand{\pref}[1]{{(\ref{#1})}}
\newcommand{\bmat}[1]{{\color{red}{#1}}}

\newtheorem{defn}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{question}{Question}[section]
\newtheorem{proposition}{Proposition}[section]
\usepackage{multicol}
\usepackage{savetrees}


% Begin the actual document content here.
\begin{document}
\begin{scriptsize}
\begin{multicols}{3}

\section*{Entropy, Relative Entropy and Mutual Information}
Definitions, alternate expressions, and properties.
\begin{align}
\underbrace{H(X)}_\text{entropy}&= -\sum_{x\in X} p(x) \log\ p(x) \label{eq:entropy}\\
H(X)& \geq 0\\
H_b(X)&= (\log_b a)H_a(X)\\
H(X|Y)& \leq  H(X)\text{ eq iff ind} \label{eq:conditioning}\\
X^n&=[X_1,\ldots,X_n]\\
H(X^n)& \leq \sum_{i=1}^n H(X_i)\text{ eq iff ind} \label{eq:jointsum}\\
H(X)&\leq  \log |\chi|  \text{ eq iff iid}\label{eq:alphabetentropy}\\
H(p)&\text{ is concave in $p$}\\
\underbrace{D(p||q)}_\text{rel ent} & = \sum_x p(x) \log \frac{p(x)}{q(x)}\\
\underbrace{I(X;Y)}_\text{mut inf} & = \sum_{x\in X} \sum_{y \in Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\
H(X)&=E_p \log \frac{1}{p(X)}\\
H(X,Y)&=E_p\log \frac{1}{p(x,y)}\\
H(X|Y)&=E_p\log \frac{p(X,Y)}{p(X)p(Y)}\\
D(p||q)&=E_p\log \frac{p(X)}{q(X)}\\
I(X;Y)&=H(X)-H(X|Y)\\
&=H(Y)-H(Y|X)\\
&=H(X)+H(Y)-H(X,Y)\\
D(p||q)&\geq 0 \text{ eq iff $p=q$}\\
I(X;Y)&=D(p(x,y)||p(x)p(y))\\
	&\geq 0 \text{ eq iff $p(x,y)=p(x)p(y)$}
\end{align}
Given $|\chi|=m$ and $u$ is the uniform distribution over $\chi$, we get equation~\ref{eq:chimu}.
\begin{align}
D(p||u)&=\log m - H(p)\label{eq:chimu}\\
D(p||q)&\text{ is convex in the pair }(p,q)
\end{align}
Following are chain rules:
\begin{align}
H(X^n)&=\sum_{i=1}^n H(X_i|X^{i-1})\\
I(X^n;Y)&=\sum_{i=1}^n I(X_i;Y|X^{i-1})\\
D(p(x,y)||q(x,y))&=D(p(x)||q(x))\\
&+D(p(y|x)||q(y|x))
\end{align}
{\bf Jensen's Inequality:} if $f$ is a convex function, then
\begin{equation}
Ef(x) \geq f(EX)
\end{equation}
{\bf Log sum inequality:} for n positive numbers $a_1,\ldots,a_n$ and $b_1,\ldots,b_n$,
\begin{equation}
\sum_{i=1}^n a_i \log \frac{a_i}{b_i} \geq \left( \sum_{i=1}^n a_i \right) \log \frac{\sum_{i=1}^n a_i}{\sum_{i=1}^n b_i}
\end{equation}
with equality only if $\frac{a_i}{b_i}$ is constant. 

{\bf Data processing inequality:} given some Markov processes $X\rightarrow Y\rightarrow Z$ we know $I(X;Y)\geq I(X;Z)$

{\bf Sufficient Statistic:} $T(X)$ is sufficient relative to $\{f_\theta (x)\}$ iff $I(\theta;X)=I(\theta;T(X))$ for all distributions on $\theta$.

{\bf Fano's inequality:} Let $P_e=Pr\{\hat{X}(Y)\neq X\}$. Then $H(P_e)+P_e \log |\chi| \geq H(X|Y)$.

{\bf Inequality.} If $X$ and $X'$ are independent and identically distributed then $Pr(X=X') \geq 2^{-H(X)}$


\section*{Asymptotic Equipartition Property (AEP)}
``Almost all events are almost equally surprising''. Specifically if $X_1,\ldots,X_n$ are iid with respect to $p(x)$ then :
\begin{equation}
-\frac{1}{n}\log p(X_1,\ldots,X_n)\rightarrow H(X) \text{in probability}
\end{equation}

A typical set $A_\epsilon^{(n)}$ is the set of sequences $x_1,\ldots, x_n$ that follow:
\begin{equation}
2^{-n(H(X)+\epsilon)}\leq p(X^n)\leq 2^{-n(H(X)-\epsilon)}
\end{equation}

Some properties of a typical set:
\begin{enumerate}
\item if $(x_1,\ldots,x_n) \in A_\epsilon^{(n)}$ then $p(x_1,\ldots,x_n) = 2^{-n(H\pm \epsilon)}$
\item Pr$\left\{A_\epsilon^{(n)}\right\}>1-\epsilon$ for sufficiently large $\epsilon$
\item $|A_\epsilon^{(n)}| \leq 2^{n(H(X)+\epsilon}$
\end{enumerate}

Definition of $a_n \doteq b_n$ means that $\frac{1}{n}\log \frac{a_n}{b_n} \rightarrow 0$ as $n\rightarrow \infty$.

Smallest probable set: Let $X_1,\ldots, X_n$ be iid with respect to $p(x)$ and for $\gamma < \frac{1}{2}$, let $B_\gamma^{(n)} \subset \chi^n$ be the smallest set such that $Pr\{ B_\gamma^{(n)} \} \geq 1-\gamma $ Then we have $|B_\gamma^{(n)}| \doteq 2^{nH}$


\section*{Channel capacity}
{\bf Channel capacity.} The logarithm of the number of distinguishable inputs is given by $ C= \text{max}_{p(x)} I(X;Y).$

{\bf Common examples} Binary symmetric channel: $C= 1 - H(p)$. Binary erasure channel: $C = 1 - \alpha$. Symmetric channel: $C= \log |\gamma | - H(\text{row of transition matrix})$

{\bf Properties of C}  1. $0 \leq C \leq \text{min}\{\log |\chi| - \log | \gamma | \}$  2. $|(X;Y)$ is a continuous concave function of $p(x)$.

{\bf Joint typicality.}  The set $A_{\epsilon}^{(n)}$ of {\it jointly typical} sequences $\{(x^{n},y^{n})\}$ with respect to the distribution $p(x,y)$ is given by $A_{\epsilon}^{(n)} = \{ (x^{n},y^{n}) \in \sigma^{n} \times \chi^{n}: \left| -\frac{1}{n}\log p(x^{n}) - H(X) \right| < \epsilon, $ $\left| -\frac{1}{n}\log p(y^{n}) - H(Y) \right| <\epsilon, $ $\left| -\frac{1}{n} \log p(x^{n},y^{n})-H(X,Y) \right| <\epsilon \}, $ where $p(x^{n},y^{n}) = \prod_{i=1}^{n}p(x_{i},y_{i}).$




\section*{Other useful laws and properties}
{\bf General Statistical things} Expected value $E(x)=\sum_{i=1}^n x_i p(x_i)$

Variance: $\sigma^2 = E\left[ \left(X-\mu\right)^2\right]$

{\bf Law of Large Numbers} Weak law:
\begin{eqnarray*}
\bar{X}_n\overset{p}\rightarrow \mu \text{ when } n \rightarrow \infty\\
\lim_{n\rightarrow \infty} Pr\left(|\bar{X}_n-\mu| < \epsilon \right) = 1
\end{eqnarray*}
Strong law:
\begin{equation}
Pr\left(\lim_{n\rightarrow \infty} \bar{X}_n = \mu \right) = 1
\end{equation}

{\bf Derivation Rules} Chain rule: $(f \circ g)'(x)=f'(g(x))g'(x)$

Product Rule: $(f\cdot g)' = f' \cdot g + f \cdot g'$

Quotient Rule: \[f(x)=\frac{g(x)}{h(x)}\text{ and }f'(x)=\frac{g'(x)h(x)-g(x)h'(x)}{\left(h(x)\right)^2}\]




\end{multicols}
\end{scriptsize}
\end{document}
