\documentclass[10pt]{article}

\usepackage{fullpage,latexsym,amsthm,amsmath,amsfonts,algorithmic,graphicx,subfigure,color,verbatim,epsfig,fancyhdr,}

% Define new commands.
\DeclareMathOperator*{\E}{\mbox{\text{E}}}

\newcommand{\pref}[1]{{(\ref{#1})}}
\newcommand{\bmat}[1]{{\color{red}{#1}}}

\newtheorem{defn}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{question}{Question}[section]
\newtheorem{proposition}{Proposition}[section]
\usepackage{multicol}
\usepackage{savetrees}

% Begin the actual document content here.
\begin{document}
\begin{tiny}
\begin{multicols}{2}
\section*{3.1}
$\mathbf{(a)}$
(Markov's Inequality) For non-negative random variable $X$ and  $t>0$,
Show
\begin{equation}
    \Pr\{X \geq t\} \leq \frac{\E X}{t} \label{eq: markovs ineq}
\end{equation}
And provide an example random variable for with the above condition is met
with equality.
\proof
\begin{eqnarray}
    \E X %&= \int_{-\infty}^\infty x f(x)dx &\text{(Defn of expectation.)}
        %\nonumber \\
    &\geq \int_t^\infty x f(x)dx &\text{(Integral over subdomain)} 
    \nonumber \\
    &\geq \int_t^\infty t f(x)dx &\text{($x \geq t$ in integral)} 
        \nonumber \\
    &= t \Pr\{X \geq t\}  &\text{($t$ constant in integral)}
        \label{eq: 3.1a QED}
\end{eqnarray}
The last equation \pref{eq: 3.1a QED} proves Markov's Inequality
\pref{eq: markovs ineq}. $\Box$

Let $X$ be a \emph{random variable} which has value zero with probability
one.
The expected value of $X$ is zero and thus, 
\pref{eq: markovs ineq} is always an equality.

$\mathbf{(b)}$
(Chebychev's Inequality) Let $Y$ be a random variable with mean $\mu$ and
variance $\sigma^2$. By letting $X = (Y-\mu)^2$, show that for
$\varepsilon > 0$
\begin{equation}
    \Pr \{|Y-\mu| > \varepsilon\} \leq \frac{\sigma^2}{\varepsilon^2}
    \label{eq: chebychevs ineq}
\end{equation}
\proof
\begin{eqnarray}
    \Pr\{(Y-\mu)^2 \geq \varepsilon^2\} &\leq& \frac{\sigma^2}{\varepsilon^2}
        \label{eq: apply markov} \\
    \Pr\{|Y-\mu| > \varepsilon\} &=& \Pr\{(Y-\mu)^2 > \varepsilon^2\} \\
    &\leq& \Pr\{(Y-\mu)^2 \geq \varepsilon^2\}
        \label{eq: compare to strict inequality}
\end{eqnarray}
\pref{eq: apply markov} is a direct application of Markov's inequality
\pref{eq: markovs ineq}.
Chebyshev's inequality \pref{eq: chebychevs ineq} comes directly from
combining \pref{eq: apply markov} and \pref{eq: compare to strict inequality}. $\Box$

$\mathbf{(c)}$
(The weak law of large numbers.) Let $Z_1,Z_2,\dots,Z_n$ be a sequence of
i.i.d. random variables with mean $\mu$ and variance $\sigma^2$. Let
$\overline{Z_n} = \frac{1}{n}\sum_{i=1}^{n}$ $Z_i$ be the sample mean.
Show 
\begin{equation}
    \Pr \{|\overline{Z_n} - \mu| > \varepsilon\} \leq
    \frac{\sigma^2}{n\varepsilon^2}
    \label{eq: weak loln}
\end{equation}
\proof
$\overline{Z_n}$ is a random variable with mean $\mu$ and variance
$\frac{\sigma^2}{n}$. This is easily computed because expectation
distributes across linear combinations, 
%\begin{equation*}
%    E\bigg[\frac{1}{n}\sum_{i=1}^{n}Z_i\bigg] 
%    = \frac{1}{n} \sum_{i=0}^{n} E[Z_i] = \frac{n\mu}{n} = \mu
%\end{equation*}
, and variance distributes across linear combinations, squaring the
leading coefficients,
%\begin{equation*}
%    Var\bigg[\frac{1}{n}\sum_{i=1}^{n}Z_i\bigg]
%    = \frac{1}{n^2}\sum_{i=0}^{n} Var[Z_i] 
%    = \frac{n\sigma^2}{n^2} 
%    = \frac{\sigma^2}{n}
%\end{equation*}
. The weak law of large numbers \pref{eq: weak loln} comes from applying
Chebychev's inequality \pref{eq: chebychevs ineq} to $\overline{Z_n}$.
$\Box$

\section*{3.2}
Let $(X_i,Y_i)$ be i.i.d. $\sim p(x,y)$. We form the log likelihood ratio
of the hypothesis that $X$ and $Y$ are independent vs. the hypothesis that
$X$ and $Y$ are dependent. What is the limit of
\begin{equation}
    \frac{1}{n}\log \frac{p(X^n)p(Y^n)}{p(X^n,Y^n)}
    \label{eq: normalized llr}
\end{equation}
$Solution$:
\begin{eqnarray}
    \frac{1}{n}\log \frac{p(X^n)p(Y^n)}{p(X^n,Y^n)}
    &=& \frac{1}{n} \sum_{i=1}^{n} \log \frac{p(X_i)p(Y_i)}{p(X_i,Y_i)}
    \label{eq: log of prods} \\
    &\rightarrow& -I(X;Y) 
    \label{eq: from MI defn}
\end{eqnarray}

The independence assumption makes \pref{eq: normalized llr} the log of a
product, yielding the sum of logs in \pref{eq: log of prods}.
The convergence in \pref{eq: from MI defn} comes from the convergence of
\pref{eq: log of prods} to $\E \log \frac{p(X_i)p(Y_i)}{p(X_i,Y_i)}$ by
the (strong?) law of large numbers. \pref{eq: from MI defn} comes from the
definition of mutual information.

This implies convergence of the likelihood ratio
$\frac{p(X^n)p(Y^n)}{p(X^n,Y^n)} \rightarrow 2^{-nI(X;Y)}$ which is 1 iff
$X$ and $Y$ are independent.
\section*{3.6}
Let $X_1,X_2,\dots$ be i.i.d. drawn according to probability mass function
$p(x)$. Find the limit $\lim_{n \to \infty}
(p(X_1,X_2,\dots,X_n))^{\frac{1}{n}}$.
$Solution$:
\begin{eqnarray}
    P_n &=& (p(X_1,X_2,\dots,X_n))^{\frac{1}{n}}
        \label{eq: defn P_n} \\
    \lim_{n\to \infty} P_n &=& 2^{lim_{n\to\infty}log\ P_n}
        \label{eq: limit of composition} \\
    &=& 2^{-H(X)},
        \label{eq: P_n limit} 
    \end{eqnarray} 
    The convergence of the exponent in \pref{eq: limit of composition} to
    $-H(X)$ comes from the AEP.
    The above result assumes that $H(X)$ exists.
\section*{3.11}
\textbf{(a)}
Given any two sets $A$, $B$ such that $\Pr{A} > 1-\varepsilon_1$ and
$\Pr{B} > 1-\varepsilon_2$, show that $\Pr(A\cap B) > 1 - \varepsilon_1 -
\varepsilon_2$. Hence
\begin{equation}
    \Pr(A_\varepsilon^{(n)} \cap B_\delta^{(n)}) > 1 - \varepsilon - \delta
    \label{eq: 3.11(a) conclusion}
\end{equation}
\proof
\begin{eqnarray}
    Pr(A\cap B) &=& Pr(A) + Pr(B) - Pr(A\cup B)
    \label{eq: pr A intersect B} \\
    &>& 2 - \varepsilon_1 - \varepsilon_2 - Pr(A\cup B)
    \label{eq: sub hypotheses} \\
    &>& 1 - \varepsilon_1 - \varepsilon_2
    \label{eq: 3.11a}
\end{eqnarray}
\textbf{(b)} 
Inequality \pref{eq: 3.11(a) conclusion} gives $1-\varepsilon-\delta \leq \Pr(A_\varepsilon^{(n)} \cap B_\delta^{(n)})$. 
The definition of probability for an event makes the RHS equal to
$\sum_{A_\varepsilon^{(n)} \cap B_\delta^{(n)}} p(x^n)$.
This value is at most 
$\sum_{A_\varepsilon^{(n)} \cap B_\delta^{(n)}} 2^{-n(H-\varepsilon)}$, 
by the definition of typical set $A_\varepsilon^{(n)}$
($x^n \in A_\varepsilon^{(n)}$ iff $2^{-n(H + \varepsilon)} \leq p(x^n) \leq 2^{-n(H - \varepsilon)}$).
The sum in carried out over a constant summand, and is equal to
$|A_\varepsilon^{(n)} \cap B_\delta^{(n)}|2^{-n(H-\varepsilon)}$, which is
upperbounded by $|B_\delta^{(n)}|2^{-n(H-\varepsilon)}$, because the
cardinality of a set intersection is at most the cardinality of either
set.

\end{multicols}
\end{tiny}
\end{document}
